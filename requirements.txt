torch==2.6.0
transformers
evaluate
dataset
pandas
sentencepiece
tqdm
PyYAML
python-dotenv
wheel
packaging
wandb
huggingface-hub
muon_optimizer
https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1+cu124torch2.6-cp310-cp310-linux_x86_64.whl
